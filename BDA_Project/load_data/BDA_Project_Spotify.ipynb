{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u6qMaetsncol",
        "outputId": "a2522c42-dd34-4859-c196-27e8b016ab64"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting pymongo\n",
            "  Downloading pymongo-4.7.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (670 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/670.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m235.5/670.0 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m665.6/670.0 kB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m670.0/670.0 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting dnspython<3.0.0,>=1.16.0 (from pymongo)\n",
            "  Downloading dnspython-2.6.1-py3-none-any.whl (307 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/307.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m307.7/307.7 kB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: dnspython, pymongo\n",
            "Successfully installed dnspython-2.6.1 pymongo-4.7.2\n"
          ]
        }
      ],
      "source": [
        "!pip install pymongo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o4sZXqECt9pe"
      },
      "source": [
        "#Phase 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        },
        "id": "V6x0jjG2eZe_",
        "outputId": "0711d48c-5f02-4302-953a-ef4031fa548a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting dataset...\n"
          ]
        },
        {
          "ename": "NameError",
          "evalue": "name 'fma_small' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-8fcef9e16df5>\u001b[0m in \u001b[0;36m<cell line: 33>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;31m# Extract the dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Extracting dataset...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mzipfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mZipFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfma_small\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mzip_ref\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m     \u001b[0mzip_ref\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextractall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mextract_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Extraction complete.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'fma_small' is not defined"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pymongo\n",
        "import librosa\n",
        "import os\n",
        "import zipfile\n",
        "import urllib.request\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Step 1: Download and Extract the Dataset\n",
        "# Define the URL and filename of the dataset\n",
        "dataset_url = \"https://www.kaggle.com/datasets/aaronyim/fma-small\"\n",
        "dataset_filename = \"fma_small.zip\"\n",
        "\n",
        "# Define the directory to extract the dataset\n",
        "extract_dir = \"fma_small\"\n",
        "\n",
        "# Check if the dataset zip file already exists\n",
        "if not os.path.exists(dataset_filename):\n",
        "    # If the dataset zip file does not exist, download it\n",
        "    print(\"Downloading dataset...\")\n",
        "    urllib.request.urlretrieve(dataset_url, dataset_filename)\n",
        "    print(\"Download complete.\")\n",
        "\n",
        "# Check if the extraction directory already exists\n",
        "if not os.path.exists(extract_dir):\n",
        "    # If the extraction directory does not exist, create it\n",
        "    os.makedirs(extract_dir)\n",
        "\n",
        "# Extract the dataset\n",
        "print(\"Extracting dataset...\")\n",
        "with zipfile.ZipFile(fma_small, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_dir)\n",
        "print(\"Extraction complete.\")\n",
        "\n",
        "# Step 2: Load the Dataset\n",
        "with zipfile.ZipFile('fma_metadata.zip', 'r') as zip_ref:\n",
        "    zip_ref.extractall('fma_small')\n",
        "\n",
        "# Load metadata\n",
        "metadata_df = pd.read_csv('fma_small/tracks.csv')\n",
        "\n",
        "# Step 3: Feature Extraction\n",
        "def extract_features(audio_file):\n",
        "    # Load audio file\n",
        "    y, sr = librosa.load(audio_file)\n",
        "\n",
        "    # Extract MFCC (Mel-Frequency Cepstral Coefficients)\n",
        "    mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13)  # Extract 13 MFCC coefficients\n",
        "\n",
        "    # Extract spectral centroid\n",
        "    spectral_centroid = librosa.feature.spectral_centroid(y=y, sr=sr)\n",
        "\n",
        "    return mfcc, spectral_centroid\n",
        "\n",
        "# Apply feature extraction to each audio file and store in a DataFrame\n",
        "features_df = pd.DataFrame(columns=['track_id', 'mfcc', 'spectral_centroid'])\n",
        "for track_id, audio_file in metadata_df[['track_id', 'path']].iterrows():\n",
        "    mfcc, spectral_centroid = extract_features(os.path.join('fma_dataset', 'fma_small', 'fma_small', '{:06d}'.format(track_id), '{:06d}.mp3'.format(track_id)))\n",
        "    features_df = features_df.append({'track_id': track_id, 'mfcc': mfcc.tolist(), 'spectral_centroid': spectral_centroid.tolist()}, ignore_index=True)\n",
        "\n",
        "# Step 4: Normalization or Standardization\n",
        "# Apply normalization or standardization to the extracted features\n",
        "scaler = StandardScaler()\n",
        "scaled_features = scaler.fit_transform(features_df['features'].values.tolist())\n",
        "features_df['features'] = scaled_features.tolist()\n",
        "\n",
        "# Step 5: Dimensionality Reduction\n",
        "# Apply dimensionality reduction techniques\n",
        "pca = PCA(n_components=50)\n",
        "reduced_features = pca.fit_transform(features_df['features'].values.tolist())\n",
        "features_df['features'] = reduced_features.tolist()\n",
        "\n",
        "# Step 6: Storage in MongoDB\n",
        "client = pymongo.MongoClient('mongodb://localhost:27017/')\n",
        "db = client['music_recommendation']\n",
        "collection = db['audio_features']\n",
        "\n",
        "# Convert DataFrame to JSON format and insert into MongoDB collection\n",
        "for row in features_df.itertuples():\n",
        "    collection.insert_one({'track_id': row.track_id, 'mfcc': row.mfcc, 'spectral_centroid': row.spectral_centroid})\n",
        "\n",
        "# Print success message\n",
        "print(\"Data stored in MongoDB successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8CFQRTocuA0J"
      },
      "source": [
        "#Phase 2\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vJ0iBzbquCjX"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
        "from pyspark.ml.recommendation import ALS\n",
        "from pyspark.ml.evaluation import RegressionEvaluator\n",
        "from pyspark.ml.tuning import TrainValidationSplit, ParamGridBuilder\n",
        "\n",
        "# Step 1: Connect to MongoDB and retrieve the audio features data\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"MusicRecommendationModel\") \\\n",
        "    .config(\"spark.mongodb.input.uri\", \"mongodb://localhost:27017/music_recommendation.audio_features\") \\\n",
        "    .config(\"spark.mongodb.output.uri\", \"mongodb://localhost:27017/music_recommendation.recommendations\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Load audio features data from MongoDB\n",
        "audio_features_df = spark.read.format(\"com.mongodb.spark.sql.DefaultSource\").load()\n",
        "\n",
        "# Feature engineering and assembling\n",
        "feature_columns = audio_features_df.columns  # Assuming all columns except 'track_id' are features\n",
        "assembler = VectorAssembler(inputCols=feature_columns, outputCol=\"features\")\n",
        "data = assembler.transform(audio_features_df)\n",
        "\n",
        "# Splitting the data into training and testing sets\n",
        "train_data, test_data = data.randomSplit([0.8, 0.2], seed=123)\n",
        "\n",
        "# Scaling the features\n",
        "scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaled_features\")\n",
        "scaler_model = scaler.fit(train_data)\n",
        "train_data_scaled = scaler_model.transform(train_data)\n",
        "test_data_scaled = scaler_model.transform(test_data)\n",
        "\n",
        "# Step 2: Training the music recommendation model with hyperparameter tuning\n",
        "# Initialize ALS model\n",
        "als = ALS(userCol=\"track_id\", itemCol=\"track_id\", ratingCol=\"play_count\",\n",
        "          coldStartStrategy=\"drop\", nonnegative=True)\n",
        "\n",
        "# Create ParamGrid for hyperparameter tuning\n",
        "param_grid = ParamGridBuilder() \\\n",
        "    .addGrid(als.rank, [10, 20, 30]) \\\n",
        "    .addGrid(als.maxIter, [10, 20]) \\\n",
        "    .addGrid(als.regParam, [0.01, 0.1]) \\\n",
        "    .build()\n",
        "\n",
        "# Define evaluator\n",
        "evaluator = RegressionEvaluator(metricName=\"rmse\", labelCol=\"play_count\", predictionCol=\"prediction\")\n",
        "\n",
        "# Create TrainValidationSplit\n",
        "tvs = TrainValidationSplit(estimator=als,\n",
        "                            estimatorParamMaps=param_grid,\n",
        "                            evaluator=evaluator,\n",
        "                            trainRatio=0.8)\n",
        "\n",
        "# Fit TrainValidationSplit\n",
        "model = tvs.fit(train_data_scaled)\n",
        "\n",
        "# Step 3: Evaluating the recommendation model\n",
        "# Make predictions\n",
        "predictions = model.transform(test_data_scaled)\n",
        "\n",
        "# Evaluate the model\n",
        "rmse = evaluator.evaluate(predictions)\n",
        "print(\"Root Mean Squared Error (RMSE) = \" + str(rmse))\n",
        "\n",
        "# Print best parameters\n",
        "best_model = model.bestModel\n",
        "print(\"Best Rank:\", best_model.rank)\n",
        "print(\"Best MaxIter:\", best_model._java_obj.parent().getMaxIter())\n",
        "print(\"Best RegParam:\", best_model._java_obj.parent().getRegParam())\n",
        "\n",
        "# Print success message\n",
        "print(\"Music recommendation model trained and evaluated successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b8oqlGJC64hl"
      },
      "source": [
        "#Phase 3\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2oXP-jxG66Ry"
      },
      "outputs": [],
      "source": [
        "# app.py\n",
        "from flask import Flask, render_template, request\n",
        "from kafka import KafkaProducer, KafkaConsumer\n",
        "import json\n",
        "\n",
        "app = Flask(__name__)\n",
        "\n",
        "# Initialize Kafka producer and consumer\n",
        "producer = KafkaProducer(bootstrap_servers=['localhost:9092'])\n",
        "consumer = KafkaConsumer('user_activity', bootstrap_servers=['localhost:9092'], group_id='recommendation_group')\n",
        "\n",
        "# Define routes\n",
        "@app.route('/')\n",
        "def index():\n",
        "    return render_template('index.html')\n",
        "\n",
        "@app.route('/playback', methods=['POST'])\n",
        "def playback():\n",
        "    # Capture user activity (e.g., track played)\n",
        "    track_id = request.form['track_id']\n",
        "\n",
        "    # Publish user activity to Kafka topic\n",
        "    producer.send('user_activity', json.dumps({'track_id': track_id}).encode('utf-8'))\n",
        "\n",
        "    return \"Playback recorded successfully.\"\n",
        "\n",
        "@app.route('/recommendation')\n",
        "def recommendation():\n",
        "    # Consume real-time recommendation from Kafka topic\n",
        "    recommendation = []\n",
        "    for message in consumer:\n",
        "        data = json.loads(message.value.decode('utf-8'))\n",
        "        recommendation.append(data['recommendation'])\n",
        "        if len(recommendation) >= 5:\n",
        "            break\n",
        "\n",
        "    # Display recommendations to the user\n",
        "    return render_template('recommendation.html', recommendation=recommendation)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    app.run(debug=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CiKR9o597C4d"
      },
      "source": [
        "#Phase 4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tfLCHTS67EWl"
      },
      "outputs": [],
      "source": [
        "music-recommendation-system/\n",
        "│\n",
        "├── data/\n",
        "│   ├── fma_dataset/          # Extracted FMA dataset\n",
        "│   ├── fma_metadata.zip     # Metadata zip file\n",
        "│   └── tracks.csv           # Metadata CSV file\n",
        "│\n",
        "├── models/                  # Trained recommendation models\n",
        "│   ├── collaborative_filtering_model.pkl\n",
        "│   └── ann_model.pkl\n",
        "│\n",
        "├── etl_pipeline.py          # ETL pipeline script\n",
        "├── train_model.py           # Model training script\n",
        "├── app.py                   # Flask web application\n",
        "│\n",
        "├── templates/               # HTML templates for web app\n",
        "│   ├── index.html\n",
        "│   └── recommendations.html\n",
        "│\n",
        "└── README.md                # Project documentation\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
